# ğŸ•·ï¸ Web Scraper API

> **Proyecto generado con IDE Cursor utilizando mÃºltiples modelos de datos de inteligencia artificial**

API REST profesional construida con NestJS para realizar web scraping y crawling automatizado de sitios web, permitiendo la extracciÃ³n inteligente de datos estructurados y no estructurados de la web.

## ğŸ¯ Objetivo del Proyecto

Este proyecto tiene como objetivo proporcionar una **soluciÃ³n robusta y escalable** para la extracciÃ³n automatizada de datos web mediante tÃ©cnicas de scraping y crawling. La API permite a desarrolladores y analistas de datos:

- **Extraer informaciÃ³n especÃ­fica** de pÃ¡ginas web usando selectores CSS personalizados
- **Realizar crawling masivo** de sitios web completos de forma controlada
- **Procesar contenido dinÃ¡mico** generado por JavaScript usando Puppeteer
- **Obtener datos estructurados** listos para anÃ¡lisis y procesamiento
- **Automatizar la recopilaciÃ³n** de informaciÃ³n de mÃºltiples fuentes web

### ğŸš€ Generado con TecnologÃ­a AI

Este proyecto fue desarrollado utilizando el **IDE Cursor** con mÃºltiples modelos de datos de inteligencia artificial, garantizando:
- CÃ³digo optimizado y siguiendo mejores prÃ¡cticas
- Arquitectura escalable y mantenible
- DocumentaciÃ³n completa y ejemplos de uso
- Validaciones robustas y manejo de errores
- IntegraciÃ³n de tecnologÃ­as modernas

## âœ¨ CaracterÃ­sticas Principales

- ğŸŒ **Web Scraping Inteligente**: ExtracciÃ³n precisa de datos especÃ­ficos
- ğŸ•·ï¸ **Web Crawling Avanzado**: NavegaciÃ³n automÃ¡tica siguiendo enlaces
- ğŸ¯ **Selectores CSS Flexibles**: ExtracciÃ³n de contenido especÃ­fico
- âš¡ **Soporte JavaScript**: Procesamiento de pÃ¡ginas dinÃ¡micas con Puppeteer
- ğŸ“Š **API REST Completa**: Interfaz documentada con Swagger/OpenAPI
- âœ… **ValidaciÃ³n Robusta**: ValidaciÃ³n automÃ¡tica de parÃ¡metros
- ğŸ›¡ï¸ **Filtros Avanzados**: Patrones regex + **filtrado por palabras clave en enlaces**
- ğŸ” **Crawling Inteligente**: Solo sigue enlaces que contengan frases especÃ­ficas
- ğŸ¯ **Control de Parada por Objetivos**: Detiene automÃ¡ticamente al alcanzar nÃºmero objetivo de pÃ¡ginas filtradas
- ğŸ“Š **ExportaciÃ³n a Excel**: Genera reportes completos con Ã¡rbol jerÃ¡rquico del sitio y anÃ¡lisis detallado
- ğŸ—‚ï¸ **PestaÃ±as Organizadas**: Cada pÃ¡gina filtrada en su propia pestaÃ±a con anÃ¡lisis granular
- ğŸ“ˆ **MÃ©tricas Detalladas**: InformaciÃ³n de rendimiento y estadÃ­sticas completas
- âš¡ **Procesamiento AsÃ­ncrono**: Jobs en background para sitios grandes con seguimiento en tiempo real
- ğŸ”§ **Timeouts Robustos**: ConfiguraciÃ³n automÃ¡tica de timeouts para sitios lentos
- ğŸš€ **OptimizaciÃ³n de Performance**: NavegaciÃ³n optimizada y manejo inteligente de recursos

## ğŸ› ï¸ Stack TecnolÃ³gico

- **NestJS** - Framework Node.js enterprise-grade
- **Puppeteer** - AutomatizaciÃ³n y control de navegador
- **Cheerio** - Parser HTML rÃ¡pido del lado servidor
- **Axios** - Cliente HTTP con interceptors
- **Swagger/OpenAPI** - DocumentaciÃ³n automÃ¡tica de API
- **class-validator** - ValidaciÃ³n de DTOs
- **TypeScript** - Tipado estÃ¡tico y desarrollo robusto

## InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone <url-del-repositorio>
cd web-scraper-api
```

### 2. Instalar dependencias

```bash
npm install
```

### 3. Configurar variables de entorno

```bash
cp .env.example .env
```

Edita el archivo `.env` con tus configuraciones especÃ­ficas.

### 4. Ejecutar la aplicaciÃ³n

#### Desarrollo
```bash
npm run start:dev
```

#### ProducciÃ³n
```bash
npm run build
npm run start:prod
```

## Uso

### DocumentaciÃ³n API

Una vez que la aplicaciÃ³n estÃ© ejecutÃ¡ndose, puedes acceder a la documentaciÃ³n interactiva de Swagger en:

```
http://localhost:3000/api
```

## ğŸ“š DocumentaciÃ³n de API

### ğŸ”— Endpoints Disponibles

#### 1. ğŸ¥ Health Check - VerificaciÃ³n del Estado del Servicio

**Objetivo**: Verificar que el servicio estÃ© funcionando correctamente y obtener informaciÃ³n bÃ¡sica del sistema.

```http
GET /scraper/health
```

**Request**: No requiere parÃ¡metros

**Response**:
```json
{
  "status": "OK",
  "message": "Servicio de web scraping funcionando correctamente",
  "timestamp": "2024-01-15T10:30:00.000Z"
}
```

**CÃ³digos de Estado**:
- `200 OK`: Servicio funcionando correctamente

---

#### 2. ğŸŒ Web Scraping - ExtracciÃ³n de Datos de URL EspecÃ­fica

**Objetivo**: Extraer datos especÃ­ficos de una pÃ¡gina web utilizando selectores CSS personalizados, con soporte para contenido estÃ¡tico y dinÃ¡mico.

```http
POST /scraper/scrape
```

**Request Body**:
```json
{
  "url": "https://example.com",
  "selectors": ["h1", ".content", "#main", "meta[name='description']"],
  "executeJavaScript": false,
  "waitTime": 1000,
  "headers": {
    "User-Agent": "Mozilla/5.0 (Custom Bot)"
  }
}
```

**ParÃ¡metros**:
- `url` (string, requerido): URL del sitio web a scrapear
- `selectors` (array, opcional): Lista de selectores CSS para extraer contenido especÃ­fico
- `executeJavaScript` (boolean, opcional): Si debe ejecutar JavaScript (usa Puppeteer)
- `waitTime` (number, opcional): Tiempo de espera en milisegundos (0-30000)
- `headers` (object, opcional): Headers HTTP personalizados

**Response**:
```json
{
  "url": "https://example.com",
  "title": "PÃ¡gina de Ejemplo",
  "content": {
    "h1": ["TÃ­tulo Principal", "SubtÃ­tulo"],
    ".content": ["Contenido del artÃ­culo..."],
    "#main": ["Contenido principal..."]
  },
  "links": [
    "https://example.com/page1",
    "https://example.com/page2",
    "/relative-link"
  ],
  "images": [
    "https://example.com/image1.jpg",
    "https://example.com/image2.png"
  ],
  "timestamp": "2024-01-15T10:30:00.000Z",
  "metadata": {
    "responseTime": 1500,
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8"
  }
}
```

**CÃ³digos de Estado**:
- `200 OK`: Scraping exitoso
- `400 Bad Request`: URL invÃ¡lida o parÃ¡metros incorrectos
- `500 Internal Server Error`: Error durante el scraping

---

#### 3. ğŸ•·ï¸ Web Crawling - Rastreo Masivo de Sitios Web

**Objetivo**: Realizar crawling automÃ¡tico de mÃºltiples pÃ¡ginas de un sitio web, siguiendo **solo enlaces que contengan frases especÃ­ficas** de forma inteligente con filtros avanzados y lÃ­mites de profundidad.

> **ğŸ” Enfoque Inteligente**: Este endpoint utiliza un sistema de filtrado por palabras clave que analiza tanto la URL como el texto del enlace, permitiendo un crawling mÃ¡s preciso y eficiente al seguir Ãºnicamente enlaces relevantes.

```http
POST /scraper/crawl
```

**Request Body**:
```json
{
  "baseUrl": "https://blog.example.com",
  "maxDepth": 3,
  "maxPages": 50,
  "includePatterns": [
    "^https://blog\\.example\\.com/\\d{4}/.*",
    "^https://blog\\.example\\.com/category/.*"
  ],
  "excludePatterns": [
    "^https://blog\\.example\\.com/admin/.*",
    "^https://blog\\.example\\.com/private/.*"
  ],
  "linkKeywords": ["artÃ­culo", "post", "blog", "noticia"],
  "excludeKeywords": ["admin", "login", "delete", "edit"],
  "targetFilteredPages": 15,
  "selectors": ["h1", ".post-content", ".author", ".publish-date"]
}
```

**ParÃ¡metros**:
- `baseUrl` (string, requerido): URL base para iniciar el crawling
- `maxDepth` (number, opcional): Profundidad mÃ¡xima del crawling (1-5, default: 2)
- `maxPages` (number, opcional): NÃºmero mÃ¡ximo de pÃ¡ginas a procesar (1-100, default: 10)
- `includePatterns` (array, opcional): Patrones regex de URLs a incluir
- `excludePatterns` (array, opcional): Patrones regex de URLs a excluir
- `linkKeywords` (array, opcional): **Frases que deben contener los enlaces para ser seguidos**
- `excludeKeywords` (array, opcional): **Frases que deben excluirse de los enlaces**
- `targetFilteredPages` (number, opcional): **NÃºmero objetivo de pÃ¡ginas filtradas para detener automÃ¡ticamente** (0-50, default: 0 = sin lÃ­mite)
- `selectors` (array, opcional): Selectores CSS para extraer datos especÃ­ficos

**Response**:
```json
{
  "baseUrl": "https://blog.example.com",
  "pages": [
    {
      "url": "https://blog.example.com",
      "title": "Blog Principal",
      "content": {
        "h1": ["Bienvenido al Blog"],
        ".post-content": ["Contenido del post..."],
        ".author": ["Juan PÃ©rez"],
        ".publish-date": ["2024-01-15"]
      },
      "links": ["https://blog.example.com/post-1", "https://blog.example.com/post-2"],
      "images": ["https://blog.example.com/hero.jpg"],
      "timestamp": "2024-01-15T10:30:00.000Z",
      "metadata": {
        "responseTime": 1200,
        "statusCode": 200,
        "contentType": "text/html"
      }
    }
  ],
       "summary": {
       "totalPages": 25,
       "filteredPages": 15,
       "targetFilteredPages": 15,
       "reachedTarget": true,
       "totalLinks": 156,
       "totalImages": 89,
       "crawlDuration": 45000
     }
}
```

**CÃ³digos de Estado**:
- `200 OK`: Crawling completado exitosamente
- `400 Bad Request`: URL base invÃ¡lida o parÃ¡metros incorrectos
- `500 Internal Server Error`: Error durante el crawling

---

#### 4. âš¡ Web Crawling AsÃ­ncrono - Para Sitios Grandes

**Objetivo**: Iniciar crawling en background para sitios grandes o procesos que pueden tomar mucho tiempo, devolviendo inmediatamente un jobId para consultar el progreso.

> **ğŸš€ Recomendado para sitios grandes**: Para sitios con mÃ¡s de 20 pÃ¡ginas o procesos que pueden tardar mÃ¡s de 30 segundos, use este endpoint para mejor experiencia de usuario.

```http
POST /scraper/crawl-async
```

**Request Body**: (Mismos parÃ¡metros que `/crawl`)
```json
{
  "baseUrl": "https://www.anzmanga25.com/",
  "maxPages": 100,
  "linkKeywords": ["one-piece", "capitulo"],
  "processChImages": true,
  "downloadImages": true
}
```

**Response (202 Accepted)**:
```json
{
  "jobId": "crawl_1640995200000_abc123def",
  "message": "Crawling iniciado. Use el jobId para consultar el progreso."
}
```

##### 4.1. Consultar Estado del Job

```http
GET /scraper/job/{jobId}
```

**Response**:
```json
{
  "jobId": "crawl_1640995200000_abc123def",
  "status": "running",
  "progress": {
    "currentPage": 15,
    "totalPages": 100,
    "filteredPages": 8,
    "currentUrl": "https://www.anzmanga25.com/manga/one-piece/capitulo-1095",
    "message": "Procesando pÃ¡gina 15/100: https://www.anzmanga25.com/..."
  },
  "startTime": "2024-01-15T10:30:00.000Z",
  "result": null // Disponible cuando status = "completed"
}
```

**Estados posibles**:
- `pending`: Job en cola esperando procesamiento
- `running`: Job ejecutÃ¡ndose actualmente
- `completed`: Job completado exitosamente (resultado disponible)
- `failed`: Job fallÃ³ (error disponible)

##### 4.2. Listar Todos los Jobs

```http
GET /scraper/jobs?status=running
```

**Response**:
```json
[
  {
    "jobId": "crawl_1640995200000_abc123def",
    "status": "running",
    "progress": {...},
    "startTime": "2024-01-15T10:30:00.000Z"
  },
  {
    "jobId": "crawl_1640995300000_xyz789ghi",
    "status": "completed",
    "progress": {...},
    "startTime": "2024-01-15T10:25:00.000Z",
    "endTime": "2024-01-15T10:28:30.000Z"
  }
]
```

##### 4.3. Limpiar Jobs Completados

```http
POST /scraper/jobs/cleanup
```

**Response**:
```json
{
  "message": "Jobs limpiados exitosamente"
}
```

---

#### 5. ğŸ“Š ExportaciÃ³n a Excel - AnÃ¡lisis Completo del Sitio

**Objetivo**: Realizar crawling completo del sitio web y generar un archivo Excel con **representaciÃ³n en Ã¡rbol del sitio filtrado** y **anÃ¡lisis detallado de cada pÃ¡gina** en pestaÃ±as separadas.

> **ğŸ¯ Funcionalidad Avanzada**: Combina crawling inteligente con generaciÃ³n automÃ¡tica de reportes en Excel, incluyendo estructura jerÃ¡rquica del sitio y anÃ¡lisis granular de cada pÃ¡gina filtrada.

```http
POST /scraper/export-excel
```

**Request Body**:
```json
{
  "baseUrl": "https://blog.example.com",
  "maxDepth": 3,
  "maxPages": 50,
  "linkKeywords": ["artÃ­culo", "tutorial", "guÃ­a"],
  "excludeKeywords": ["admin", "login"],
  "targetFilteredPages": 20,
  "selectors": ["h1", ".content", ".author", ".publish-date"],
  "includeDetailedAnalysis": true,
  "fileName": "analisis-sitio-web"
}
```

**ParÃ¡metros**:
- `baseUrl` (string, requerido): URL base para el anÃ¡lisis
- `maxDepth` (number, opcional): Profundidad mÃ¡xima (1-5, default: 2)
- `maxPages` (number, opcional): PÃ¡ginas mÃ¡ximas a procesar (1-100, default: 20)
- `linkKeywords` (array, opcional): Palabras clave para filtrar enlaces
- `excludeKeywords` (array, opcional): Palabras clave para excluir
- `targetFilteredPages` (number, opcional): NÃºmero objetivo de pÃ¡ginas filtradas
- `selectors` (array, opcional): Selectores CSS para extracciÃ³n detallada
- `includeDetailedAnalysis` (boolean, opcional): Incluir pestaÃ±as detalladas (default: true)
- `fileName` (string, opcional): Nombre del archivo Excel

**Response**: Archivo Excel con las siguientes pestaÃ±as:

1. **ğŸŒ³ Ãrbol del Sitio**: RepresentaciÃ³n jerÃ¡rquica completa del sitio web filtrado
2. **ğŸ“Š Resumen**: EstadÃ­sticas generales y anÃ¡lisis por profundidad
3. **ğŸ“„ PÃ¡ginas Individuales**: Una pestaÃ±a por cada pÃ¡gina filtrada con anÃ¡lisis detallado

**CÃ³digos de Estado**:
- `200 OK`: Archivo Excel generado exitosamente
- `400 Bad Request`: ParÃ¡metros de exportaciÃ³n invÃ¡lidos
- `500 Internal Server Error`: Error durante la generaciÃ³n del reporte

## ğŸ’» Ejemplos PrÃ¡cticos de Uso

### ğŸ” Ejemplo 1: Scraping de Noticias

```javascript
// Extraer tÃ­tulos y puntuaciones de Hacker News
const response = await fetch('http://localhost:3000/scraper/scrape', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    url: 'https://news.ycombinator.com',
    selectors: ['.titleline > a', '.score'],
    executeJavaScript: false
  })
});

const newsData = await response.json();
console.log('TÃ­tulos extraÃ­dos:', newsData.content['.titleline > a']);
console.log('Puntuaciones:', newsData.content['.score']);
```

### ğŸ“° Ejemplo 2: Crawling Inteligente de Blog

```javascript
// Crawlear un blog siguiendo solo enlaces con palabras clave especÃ­ficas
const response = await fetch('http://localhost:3000/scraper/crawl', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    baseUrl: 'https://blog.example.com',
    maxDepth: 3,
    maxPages: 25,
    // Filtros por palabras clave en enlaces (NUEVO)
    linkKeywords: ['artÃ­culo', 'tutorial', 'guÃ­a', 'post'],
    excludeKeywords: ['admin', 'login', 'register', 'delete', 'edit'],
    // Detener automÃ¡ticamente al encontrar 20 pÃ¡ginas que cumplan criterios
    targetFilteredPages: 20,
    // Filtros tradicionales por URL
    includePatterns: [
      '^https://blog\\.example\\.com/\\d{4}/.*',
      '^https://blog\\.example\\.com/category/.*'
    ],
    excludePatterns: [
      '^https://blog\\.example\\.com/admin/.*'
    ],
    selectors: ['h1', '.post-content', '.author', '.publish-date']
  })
});

const crawlData = await response.json();
console.log(`âœ… Crawling completado:`);
console.log(`ğŸ“„ PÃ¡ginas procesadas: ${crawlData.summary.totalPages}`);
console.log(`ğŸ¯ PÃ¡ginas filtradas encontradas: ${crawlData.summary.filteredPages}`);
console.log(`ğŸ Objetivo alcanzado: ${crawlData.summary.reachedTarget ? 'SÃ' : 'NO'}`);
console.log(`ğŸ”— Enlaces encontrados: ${crawlData.summary.totalLinks}`);
console.log(`ğŸ–¼ï¸ ImÃ¡genes encontradas: ${crawlData.summary.totalImages}`);
console.log(`â±ï¸ Tiempo total: ${crawlData.summary.crawlDuration}ms`);
```

### ğŸ›ï¸ Ejemplo 3: Scraping de E-commerce

```javascript
// Extraer informaciÃ³n de productos
const response = await fetch('http://localhost:3000/scraper/scrape', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    url: 'https://ecommerce.example.com/product/123',
    selectors: [
      '.product-title',
      '.price',
      '.description',
      '.rating',
      '.availability'
    ],
    executeJavaScript: true,
    waitTime: 2000,
    headers: {
      'User-Agent': 'Mozilla/5.0 (Scraper Bot) WebScraper/1.0'
    }
  })
});

const productData = await response.json();
console.log('InformaciÃ³n del producto:', productData.content);
```

### ğŸ“Š Ejemplo 4: Crawling con Control de Objetivos

```javascript
// Buscar exactamente 10 artÃ­culos tÃ©cnicos, sin importar cuÃ¡ntas pÃ¡ginas se procesen
const response = await fetch('http://localhost:3000/scraper/crawl', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    baseUrl: 'https://tech-blog.example.com',
    maxPages: 200,                           // LÃ­mite mÃ¡ximo de pÃ¡ginas a procesar
    targetFilteredPages: 10,                 // OBJETIVO: encontrar exactamente 10 pÃ¡ginas relevantes
    linkKeywords: ['javascript', 'tutorial', 'programming', 'code'],
    excludeKeywords: ['advertising', 'sponsor'],
    selectors: ['h1', '.content', '.code-snippet', '.author']
  })
});

const result = await response.json();

console.log(`ğŸ¯ Objetivo: ${result.summary.targetFilteredPages} pÃ¡ginas`);
console.log(`âœ… Encontradas: ${result.summary.filteredPages} pÃ¡ginas filtradas`);
console.log(`ğŸ Completado: ${result.summary.reachedTarget ? 'Objetivo alcanzado' : 'LÃ­mite mÃ¡ximo alcanzado'}`);
console.log(`ğŸ“„ Total procesadas: ${result.summary.totalPages} pÃ¡ginas`);
```

### âš¡ Ejemplo 5: Crawling AsÃ­ncrono para Sitios Grandes

```javascript
// Para sitios grandes, usar crawling asÃ­ncrono
async function crawlLargeSite() {
  // 1. Iniciar crawling asÃ­ncrono
  const startResponse = await fetch('http://localhost:3000/scraper/crawl-async', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      baseUrl: 'https://www.anzmanga25.com/',
      maxPages: 200,
      linkKeywords: ['one-piece', 'capitulo'],
      processChImages: true,
      downloadImages: true,
      targetFilteredPages: 50
    })
  });

  const { jobId } = await startResponse.json();
  console.log(`ğŸš€ Crawling iniciado con jobId: ${jobId}`);

  // 2. Monitorear progreso
  let status = 'pending';
  while (status === 'pending' || status === 'running') {
    await new Promise(resolve => setTimeout(resolve, 5000)); // Esperar 5 segundos
    
    const statusResponse = await fetch(`http://localhost:3000/scraper/job/${jobId}`);
    const jobStatus = await statusResponse.json();
    
    status = jobStatus.status;
    console.log(`ğŸ“Š Estado: ${status}`);
    console.log(`ğŸ“„ Progreso: ${jobStatus.progress.currentPage}/${jobStatus.progress.totalPages} pÃ¡ginas`);
    console.log(`ğŸ¯ Filtradas: ${jobStatus.progress.filteredPages} pÃ¡ginas`);
    console.log(`ğŸ”— URL actual: ${jobStatus.progress.currentUrl}`);
    console.log(`ğŸ’¬ Mensaje: ${jobStatus.progress.message}`);
    console.log('---');
  }

  // 3. Obtener resultado final
  const finalResponse = await fetch(`http://localhost:3000/scraper/job/${jobId}`);
  const finalResult = await finalResponse.json();
  
  if (status === 'completed') {
    console.log('âœ… Crawling completado exitosamente!');
    console.log(`ğŸ“Š Resultado final:`, finalResult.result.summary);
    return finalResult.result;
  } else {
    console.error('âŒ Crawling fallÃ³:', finalResult.error);
    throw new Error(finalResult.error);
  }
}

// Ejecutar
crawlLargeSite()
  .then(result => console.log('Datos obtenidos:', result.pages.length, 'pÃ¡ginas'))
  .catch(error => console.error('Error:', error));
```

### ğŸ“Š Ejemplo 6: ExportaciÃ³n Completa a Excel

```javascript
// Generar reporte completo en Excel del sitio web
const response = await fetch('http://localhost:3000/scraper/export-excel', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    baseUrl: 'https://docs.example.com',
    maxDepth: 4,
    maxPages: 100,
    targetFilteredPages: 30,
    linkKeywords: ['documentation', 'guide', 'tutorial', 'api'],
    excludeKeywords: ['deprecated', 'legacy', 'old'],
    selectors: [
      'h1', 'h2', 'h3',                    // TÃ­tulos
      '.content', '.description',          // Contenido principal
      '.author', '.last-updated',          // Metadatos
      'code', '.code-block'                // CÃ³digo
    ],
    includeDetailedAnalysis: true,
    fileName: 'documentacion-completa'
  })
});

// Descargar el archivo Excel
const blob = await response.blob();
const url = window.URL.createObjectURL(blob);
const a = document.createElement('a');
a.href = url;
a.download = 'documentacion-completa.xlsx';
a.click();

console.log('ğŸ“Š Reporte Excel generado y descargado exitosamente');
```

### ğŸ”§ Ejemplo 6: AnÃ¡lisis de Redes Sociales

```bash
# Usando cURL para extraer datos de redes sociales
curl -X POST http://localhost:3000/scraper/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://social.example.com/profile/username",
    "selectors": [".post-text", ".engagement-count", ".hashtags"],
    "executeJavaScript": true,
    "waitTime": 3000
  }'
```

### ğŸ§  CÃ³mo Funciona el Filtrado Inteligente por Palabras Clave

El sistema de crawling analiza cada enlace encontrado y evalÃºa:

1. **ğŸ“ Texto del enlace**: El contenido visible del enlace (ej: "Leer artÃ­culo completo")
2. **ğŸ”— URL del enlace**: La direcciÃ³n web del enlace
3. **ğŸ¯ Palabras clave de inclusiÃ³n**: Solo sigue enlaces que contengan estas frases
4. **ğŸš« Palabras clave de exclusiÃ³n**: Evita enlaces que contengan estas frases

**Ejemplo prÃ¡ctico**:
```html
<!-- Este enlace SÃ serÃ¡ seguido -->
<a href="/blog/articulo-tutorial">Ver tutorial completo</a>

<!-- Este enlace NO serÃ¡ seguido (contiene "admin") -->
<a href="/admin/panel">Panel de administraciÃ³n</a>
```

### ğŸ¯ Control Inteligente de Parada por Objetivos

AdemÃ¡s del filtrado por palabras clave, el sistema incluye **control automÃ¡tico de parada** que permite definir exactamente cuÃ¡ntas pÃ¡ginas que cumplan los criterios deseas encontrar:

**ParÃ¡metro**: `targetFilteredPages`
- **Valor 0 (default)**: Sin lÃ­mite, continÃºa hasta alcanzar `maxPages`
- **Valor > 0**: Se detiene automÃ¡ticamente al encontrar ese nÃºmero de pÃ¡ginas filtradas

**Ejemplo prÃ¡ctico**:
```json
{
  "baseUrl": "https://news.example.com",
  "maxPages": 100,
  "targetFilteredPages": 20,
  "linkKeywords": ["noticia", "artÃ­culo"]
}
```

**Resultado**: El crawler procesarÃ¡ hasta 100 pÃ¡ginas, pero **se detendrÃ¡ automÃ¡ticamente** cuando encuentre exactamente 20 pÃ¡ginas que contengan "noticia" o "artÃ­culo", optimizando tiempo y recursos.

**Ventajas**:
- ğŸ¯ **Control preciso**: Obtienes exactamente el nÃºmero de pÃ¡ginas relevantes que necesitas
- âš¡ **Eficiencia**: Evita procesamiento innecesario una vez alcanzado el objetivo
- ğŸ“Š **Predictibilidad**: Resultados consistentes independientemente del tamaÃ±o del sitio

## ğŸ“Š Estructura del Archivo Excel Generado

### ğŸŒ³ **PestaÃ±a 1: Ãrbol del Sitio**
RepresentaciÃ³n jerÃ¡rquica visual del sitio web crawleado:

| Columna | DescripciÃ³n | Ejemplo |
|---------|-------------|---------|
| **Nivel** | Profundidad en la jerarquÃ­a | 0, 1, 2, 3... |
| **Estructura** | RepresentaciÃ³n visual del Ã¡rbol | `â”œâ”€â”€ PÃ¡gina Principal` |
| **URL** | DirecciÃ³n completa de la pÃ¡gina | `https://blog.com/post/123` |
| **TÃ­tulo** | TÃ­tulo extraÃ­do de la pÃ¡gina | `"GuÃ­a de JavaScript Avanzado"` |
| **Estado** | CÃ³digo de respuesta HTTP | `200`, `404`, `500` |
| **Tiempo (ms)** | Tiempo de respuesta | `1500` |
| **Enlaces** | NÃºmero de enlaces encontrados | `25` |
| **ImÃ¡genes** | NÃºmero de imÃ¡genes encontradas | `8` |

### ğŸ“Š **PestaÃ±a 2: Resumen Ejecutivo**
EstadÃ­sticas generales del anÃ¡lisis:

- **ğŸ“ˆ MÃ©tricas Globales**: Total de pÃ¡ginas, filtradas, objetivos alcanzados
- **ğŸ•°ï¸ InformaciÃ³n Temporal**: Fecha del anÃ¡lisis, duraciÃ³n del crawling
- **ğŸ“Š AnÃ¡lisis por Profundidad**: DistribuciÃ³n de pÃ¡ginas por nivel jerÃ¡rquico
- **ğŸ¯ Cumplimiento de Objetivos**: Porcentaje de objetivos alcanzados

### ğŸ“„ **PestaÃ±as 3+: AnÃ¡lisis Detallado por PÃ¡gina**
Una pestaÃ±a individual para cada pÃ¡gina filtrada que contiene:

#### ğŸ“‹ **InformaciÃ³n General**
- URL completa de la pÃ¡gina
- TÃ­tulo extraÃ­do
- Fecha y hora del anÃ¡lisis
- MÃ©tricas de rendimiento (tiempo de respuesta, cÃ³digo de estado)
- Tipo de contenido detectado

#### ğŸ¯ **Contenido ExtraÃ­do por Selectores**
Datos organizados por cada selector CSS utilizado:
```
Selector: h1
1. TÃ­tulo Principal de la PÃ¡gina
2. SubtÃ­tulo Importante

Selector: .content
1. PÃ¡rrafo de introducciÃ³n...
2. Contenido del artÃ­culo...

Selector: .author
1. Juan PÃ©rez
```

#### ğŸ”— **Enlaces Detectados** (hasta 50 por pÃ¡gina)
Lista numerada de todos los enlaces encontrados en la pÃ¡gina

#### ğŸ–¼ï¸ **Recursos Multimedia**
Enlaces a imÃ¡genes y otros recursos multimedia detectados

### âœ¨ **CaracterÃ­sticas Avanzadas del Excel**

- **ğŸ¨ CodificaciÃ³n por Colores**: PÃ¡ginas filtradas resaltadas en verde
- **ğŸ“Š Filtros AutomÃ¡ticos**: Cada columna incluye filtros para anÃ¡lisis rÃ¡pido
- **ğŸ“ Columnas Autoajustadas**: Anchos optimizados para mejor legibilidad
- **ğŸ·ï¸ Nombres Inteligentes**: PestaÃ±as nombradas automÃ¡ticamente basÃ¡ndose en el tÃ­tulo de la pÃ¡gina
- **ğŸ“ˆ Formato Profesional**: Estilos empresariales para presentaciones

## ConfiguraciÃ³n avanzada

### Variables de entorno

| Variable | DescripciÃ³n | Valor por defecto |
|----------|-------------|-------------------|
| `PORT` | Puerto del servidor | `3000` |
| `MAX_CONCURRENT_REQUESTS` | MÃ¡ximo de peticiones concurrentes | `5` |
| `DEFAULT_REQUEST_TIMEOUT` | Timeout por defecto (ms) | `10000` |
| `PUPPETEER_EXECUTABLE_PATH` | Ruta del ejecutable de Chrome/Chromium | Sistema |

### Selectores CSS

Puedes usar cualquier selector CSS vÃ¡lido:

- `h1, h2, h3` - TÃ­tulos
- `.class-name` - Por clase CSS
- `#element-id` - Por ID
- `[attribute="value"]` - Por atributo
- `div > p` - Selectores descendientes
- `a[href*="github"]` - Enlaces que contengan "github"

## Desarrollo

### Scripts disponibles

```bash
npm run start:dev    # Modo desarrollo con hot reload
npm run build        # Construir para producciÃ³n
npm run test         # Ejecutar tests
npm run test:watch   # Tests en modo watch
npm run lint         # Verificar cÃ³digo con ESLint
npm run format       # Formatear cÃ³digo con Prettier
```

### Testing

```bash
# Tests unitarios
npm run test

# Tests e2e
npm run test:e2e

# Coverage
npm run test:cov
```

## âš ï¸ Limitaciones y Consideraciones Ã‰ticas

### ğŸš¦ Buenas PrÃ¡cticas de Web Scraping

- **ğŸ•°ï¸ Rate Limiting**: Implementa delays entre peticiones para no sobrecargar los servidores
- **ğŸ¤– robots.txt**: Siempre verifica y respeta el archivo robots.txt del sitio web
- **ğŸ“œ TÃ©rminos de Servicio**: AsegÃºrate de cumplir con los ToS del sitio objetivo
- **âš–ï¸ Uso Responsable**: Utiliza la herramienta de forma Ã©tica y legal
- **ğŸ”’ Datos Personales**: No extraigas informaciÃ³n personal sin consentimiento
- **ğŸ›¡ï¸ DetecciÃ³n de Bots**: Algunos sitios implementan medidas anti-bot

### ğŸ“ˆ Consideraciones de Rendimiento

- **ğŸ’¾ Memoria**: El crawling de sitios grandes puede consumir memoria significativa
- **ğŸ”„ Concurrencia**: Limita las peticiones concurrentes para evitar sobrecargar el sistema
- **â±ï¸ Timeouts**: Configura timeouts apropiados para evitar bloqueos
- **ğŸ“Š Monitoreo**: Supervisa el uso de recursos durante operaciones intensivas

## ğŸš€ Despliegue y ProducciÃ³n

### ğŸ³ Docker (Recomendado)

```dockerfile
# Crear Dockerfile
FROM node:18-alpine

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

EXPOSE 3000
CMD ["npm", "run", "start:prod"]
```

```bash
# Construir y ejecutar
docker build -t web-scraper-api .
docker run -p 3000:3000 web-scraper-api
```

### â˜ï¸ Variables de Entorno para ProducciÃ³n

```bash
PORT=3000
NODE_ENV=production
MAX_CONCURRENT_REQUESTS=3
DEFAULT_REQUEST_TIMEOUT=15000
PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser
LOG_LEVEL=info
```

## ğŸ”§ PersonalizaciÃ³n y ExtensiÃ³n

### ğŸ¨ Agregando Nuevos Extractores

```typescript
// src/scraper/extractors/custom-extractor.ts
export class CustomExtractor {
  async extractData(page: Page, selectors: string[]): Promise<any> {
    // LÃ³gica personalizada de extracciÃ³n
  }
}
```

### ğŸ“¡ Integraciones Adicionales

- **ğŸ—„ï¸ Base de Datos**: MongoDB, PostgreSQL para almacenar resultados
- **ğŸ“¬ Colas**: Redis/Bull para procesamiento asÃ­ncrono
- **ğŸ“Š MÃ©tricas**: Prometheus/Grafana para monitoreo
- **ğŸ” AutenticaciÃ³n**: JWT para acceso controlado

## ğŸ¯ Casos de Uso Comunes

| Industria | Caso de Uso | Selectores TÃ­picos |
|-----------|-------------|-------------------|
| **E-commerce** | Monitoreo de precios | `.price`, `.product-title`, `.rating` |
| **Inmobiliaria** | Listados de propiedades | `.property-price`, `.address`, `.features` |
| **Noticias** | AgregaciÃ³n de contenido | `h1`, `.article-content`, `.author` |
| **Redes Sociales** | AnÃ¡lisis de engagement | `.post-text`, `.like-count`, `.share-count` |
| **InvestigaciÃ³n** | RecopilaciÃ³n acadÃ©mica | `.abstract`, `.authors`, `.citations` |
| **AuditorÃ­a SEO** | AnÃ¡lisis de estructura del sitio | `h1`, `h2`, `meta[name="description"]`, `title` |
| **DocumentaciÃ³n** | Mapeo de conocimiento | `.article-title`, `.content`, `.code-example` |
| **Competitive Intel** | AnÃ¡lisis de competencia | `.product-name`, `.price`, `.features` |

## ğŸ“Š Casos de Uso EspecÃ­ficos para Excel

### ğŸ¢ **AuditorÃ­a SEO Completa**
```javascript
{
  "baseUrl": "https://mi-sitio.com",
  "maxDepth": 5,
  "maxPages": 200,
  "selectors": ["title", "h1", "h2", "h3", "meta[name='description']", ".breadcrumb"],
  "fileName": "auditoria-seo-completa"
}
```
**Resultado**: Excel con estructura completa del sitio para anÃ¡lisis SEO, identificaciÃ³n de pÃ¡ginas sin tÃ­tulos, jerarquÃ­a de encabezados, etc.

### ğŸ“š **Mapeo de DocumentaciÃ³n TÃ©cnica**
```javascript
{
  "linkKeywords": ["documentation", "api", "guide", "tutorial"],
  "excludeKeywords": ["deprecated", "legacy"],
  "targetFilteredPages": 50,
  "selectors": [".doc-title", ".code-snippet", ".api-endpoint", ".example"]
}
```
**Resultado**: Reporte estructurado de toda la documentaciÃ³n tÃ©cnica con ejemplos de cÃ³digo organizados por secciones.

### ğŸ›ï¸ **AnÃ¡lisis de CatÃ¡logo de Productos**
```javascript
{
  "linkKeywords": ["product", "item", "catalog"],
  "selectors": [".product-name", ".price", ".description", ".rating", ".availability"],
  "targetFilteredPages": 100
}
```
**Resultado**: Base de datos completa de productos en Excel para anÃ¡lisis de precios y caracterÃ­sticas.

## ğŸ“ Soporte y Contacto

### ğŸ› Reportar Problemas

Si encuentras algÃºn problema o tienes sugerencias:

1. **ğŸ” Issues**: Abre un issue en el repositorio
2. **ğŸ“§ Email**: Contacta al equipo de desarrollo
3. **ğŸ’¬ Discussions**: Participa en las discusiones del proyecto

### ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Para contribuir:

1. **ğŸ´ Fork** el proyecto
2. **ğŸŒ¿ Branch** para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. **ğŸ’¾ Commit** tus cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. **ğŸ“¤ Push** al branch (`git push origin feature/nueva-funcionalidad`)
5. **ğŸ”„ Pull Request** para revisiÃ³n

### ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

---

<div align="center">

**ğŸ•·ï¸ Web Scraper API** - *Generado con IDE Cursor utilizando mÃºltiples modelos de IA*

![NestJS](https://img.shields.io/badge/NestJS-E0234E?style=for-the-badge&logo=nestjs&logoColor=white)
![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white)
![Puppeteer](https://img.shields.io/badge/Puppeteer-40B5A4?style=for-the-badge&logo=puppeteer&logoColor=white)

*Desarrollado con tecnologÃ­a de inteligencia artificial para mÃ¡xima eficiencia y calidad*

</div> 